Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@incollection{Shaliza2012,
abstract = {Logistic regression is a particular instance of a broader kind of model, called a gener-alized linear model (GLM). You are familiar, of course, from your regression class with the idea of transforming the response variable, what we've been calling Y , and then predicting the transformed variable from X . This was not what we did in logis-tic regression. Rather, we transformed the conditional expected value, and made that a linear function of X . This seems odd, because it is odd, but it turns out to be useful. Let's be specific. Our usual focus in regression modeling has been the condi-tional expectation function, r (x) = E [Y |X = x]. In plain linear regression, we try to approximate r (x) by $\beta$ 0 + x {\textperiodcentered} $\beta$. In logistic regression, r (x) = E [Y |X = x] = Pr (Y = 1|X = x), and it is a transformation of r (x) which is linear. The usual nota-tion says $\eta$(x) = $\beta$ 0 + xc ˙ $\beta$ (13.1) $\eta$(x) = log r (x) 1 − r (x) (13.2) = g (r (x)) (13.3) defining the logistic link function by g (m) = log m/(1 − m). The function $\eta$(x) is called the linear predictor. Now, the naive strategy for estimating this model would be to all the transforma-tion g to the response. But Y is always zero or one, so g (Y) = ±∞, and regression will not be helpful here. The standard strategy is instead to use (what else?) Taylor expansion. Specifically, we try expanding g (Y) around r (x), and stop at first order: g (Y) ≈ g (r (x)) + (Y − r (x)) g {\"{i}}¿¿ (r (x)) (13.4)},
author = {Shaliza, Cosma},
booktitle = {Advanced Data Analysis (lecture notes)},
chapter = {13},
pages = {238--257},
title = {{Generalized Linear Models and Generalized Additive Models}},
year = {2012}
}
