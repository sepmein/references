Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Socher2010,
abstract = {Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-aware recursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases “decline to comment” and “would not disclose the terms” are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1{\%} on the Wall Street Journal development dataset for sentences up to length 15.},
author = {Socher, Richard and Manning, Christopher D Cd and Ng, Andrew Y Ay},
doi = {10.1007/978-3-540-87479-9},
file = {:Users/sepmein/Qsync/Papers/Socher, Manning, Ng{\_}2010{\_}Learning continuous phrase representations and syntactic parsing with recursive neural networks.pdf:pdf},
isbn = {978-3-540-87478-2},
issn = {0302-9743},
journal = {Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
pages = {1--9},
title = {{Learning continuous phrase representations and syntactic parsing with recursive neural networks}},
url = {http://wuawua.googlecode.com/files/Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks.pdf},
year = {2010}
}
