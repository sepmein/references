Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Wei2016,
abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as $\backslash$emph{\{}network morphism{\}} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
archivePrefix = {arXiv},
arxivId = {1603.01670},
author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
eprint = {1603.01670},
file = {:Users/sepmein/Qsync/Papers/Wei et al.{\_}2016{\_}Network Morphism.pdf:pdf},
isbn = {9781510829008},
title = {{Network Morphism}},
url = {http://arxiv.org/abs/1603.01670},
year = {2016}
}
