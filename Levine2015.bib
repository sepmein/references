Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Levine2015,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:Users/sepmein/Qsync/Papers/Levine et al.{\_}2015{\_}End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
keywords = {neural networks,optimal control,reinforcement learning,vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {http://arxiv.org/abs/1504.00702},
volume = {17},
year = {2015}
}
