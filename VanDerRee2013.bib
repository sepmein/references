Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{VanDerRee2013,
abstract = {This paper compares three strategies in using reinforcement learning algorithms to let an artificial agent learn to play the game of Othello. The three strategies that are compared are: Learning by self-play, learning from playing against a fixed opponent, and learning from playing against a fixed opponent while learning from the opponent's moves as well. These issues are considered for the algorithms Q-learning, Sarsa and TD-learning. These three reinforcement learning algorithms are combined with multi-layer perceptrons and trained and tested against three fixed opponents. It is found that the best strategy of learning differs per algorithm. Q-learning and Sarsa perform best when trained against the fixed opponent they are also tested against, whereas TD-learning performs best when trained through self-play. Surprisingly, Q-learning and Sarsa outperform TD-learning against the stronger fixed opponents, when all methods use their best strategy. Learning from the opponent's moves as well leads to worse results compared to learning only from the learning agent's own moves.},
author = {{Van Der Ree}, Michiel and Wiering, Marco},
doi = {10.1109/ADPRL.2013.6614996},
file = {:Users/sepmein/Qsync/Papers/Van Der Ree, Wiering{\_}2013{\_}Reinforcement learning in the game of Othello Learning against a fixed opponent and learning from self-play.pdf:pdf},
isbn = {9781467359252},
issn = {23251824},
journal = {IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL},
pages = {108--115},
title = {{Reinforcement learning in the game of Othello: Learning against a fixed opponent and learning from self-play}},
year = {2013}
}
