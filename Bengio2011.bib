Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Bengio2011,
abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher- level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P(x) is structurally related to some task of interest, say predicting P(y|x). This paper focusses on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution},
archivePrefix = {arXiv},
arxivId = {1606.09549},
author = {Bengio, Yoshua},
doi = {10.1109/IJCNN.2011.6033302},
eprint = {1606.09549},
file = {:Users/sepmein/Qsync/Papers/Bengio{\_}2011{\_}Deep Learning of Representations for Unsupervised and Transfer Learning.pdf:pdf},
isbn = {9780971977778},
issn = {1938-7228},
journal = {JMLR: Workshop and Conference Proceedings},
keywords = {autoencoders,deep learning,domain adaptation,ing,multi-task learning,neural networks,re-,representation learning,self-taught learning,stricted boltzmann machines,transfer learn-,unsupervised learning},
pages = {1--20},
pmid = {27295638},
title = {{Deep Learning of Representations for Unsupervised and Transfer Learning}},
volume = {7},
year = {2011}
}
