Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Dayan1999,
abstract = {From a NN perspective: US1 or 0 –weight–{\textgreater} V(reward function or value function)–{\textgreater}Prediction weight is equal to expected reward. The NN approach explains: 1. Partial reinforcement Except it does not seem to explain why partial schedules work better than constant. It would seem to follow, as Kari points out, that⅟2 the reward per time would have an = effect as a partal reward sched where the reward was only given⅟2 the time. 2. Extinction 3. Multiple reinforcements.},
author = {Dayan, Peter and Abbott, Larry},
file = {:Users/sepmein/Qsync/Papers/Dayan, Abbott{\_}1999{\_}Reinforcement Learning.pdf:pdf},
number = {Teil 2},
title = {{Reinforcement Learning}},
year = {1999}
}
