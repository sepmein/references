Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Wirth2013,
abstract = {{\textcopyright} 2013 C. Wirth {\&} J. F{\"{u}}rnkranz. Reinforcement learning algorithms are usually hard to use for non expert users. It is required to consider several aspects like the definition of state-, action- And reward-space as well as the algorithms hyperparameters. Preference based approaches try to address these problems by omitting the requirement for exact rewards, replacing them with preferences over solutions. Some algorithms have been proposed within this framework, but they are usually requiring parameterized policies which is again a hinderance for their application. Monte Carlo based approaches do not have this restriction and are also model free. Hence, we present a new preference-based reinforcement learning algorithm, utilizing Monte Carlo estimates. The main idea is to estimate the relative Q-value of two actions for the same state within a every-visit framework. This means, preferences are used to estimate the Q-value of state-action pairs within a trajectory, based on the feedback concerning the complete trajectory. The algorithm is evaluated on three common benchmark problems, namely mountain car, inverted pendulum and acrobot, showing its advantage over a closely related algorithm which is also using estimates for intermediate states, but based on a probability theorem. In comparison to SARSA($\lambda$), EPMC converges somewhat slower, but computes policies that are almost as good or better.},
author = {Wirth, C. and F{\"{u}}rnkranz, J.},
file = {:Users/sepmein/Qsync/Papers/Wirth, F{\"{u}}rnkranz{\_}2013{\_}EPMC Every visit preference Monte Carlo for reinforcement learning.pdf:pdf},
issn = {15337928 15324435},
journal = {Journal of Machine Learning Research},
keywords = {Every visit,Monte Carlo,Preference learning,Reinforcement learning},
pages = {483--497},
title = {{EPMC: Every visit preference Monte Carlo for reinforcement learning}},
volume = {29},
year = {2013}
}
